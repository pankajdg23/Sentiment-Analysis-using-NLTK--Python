{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#Author :Pankaj D Gaikwad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Movie Review Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk,random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import  movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data=[(list(movie_reviews.words(fileid)),category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#Preprocessing on dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from itertools import chain\n",
    "\n",
    "from nltk.corpus import movie_reviews as mr\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier as nbc\n",
    "\n",
    "stop=list(set(stopwords.words('english')))\n",
    "#stop = stopwords.words('english')\n",
    "documents = [([w for w in mr.words(i) if w.lower() not in stop and w.lower() not in string.punctuation], i.split('/')[0]) for i in mr.fileids()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(documents)\n",
    "train=documents[:1500]\n",
    "test=documents[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocabulary=[]\n",
    "for i in range(0,len(train)):\n",
    "    vocabulary.extend(train[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535214\n",
      "35468\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))\n",
    "vocabulary=list(set(vocabulary))\n",
    "vocabulary.sort()\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "source": [
    "## Preparing unigram feature vectror based on presence/absence of words in vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_unigram_features(data,vocab):\n",
    "    fet_vec_all = []\n",
    "    for tup in data:\n",
    "        single_feat_vec = []\n",
    "        sent = tup[0][0].lower() #lowercasing the dataset\n",
    "        for v in vocab:\n",
    "            if sent.__contains__(v):\n",
    "                single_feat_vec.append(1)\n",
    "            else:\n",
    "                single_feat_vec.append(0)\n",
    "        fet_vec_all.append(single_feat_vec)\n",
    "    return fet_vec_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Add sentiment scores from sentiwordnet, here we take the average sentiment scores of all word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_senti_wordnet_features(data):\n",
    "    fet_vec_all = []\n",
    "    for tup in data:\n",
    "        words = tup[0][0].lower()\n",
    "        #words = sent.split()\n",
    "        pos_score = 0\n",
    "        neg_score = 0\n",
    "        for w in words:\n",
    "            senti_synsets = swn.senti_synsets(w.lower())\n",
    "            for senti_synset in senti_synsets:\n",
    "                p = senti_synset.pos_score()\n",
    "                n = senti_synset.neg_score()\n",
    "                pos_score+=p\n",
    "                neg_score+=n\n",
    "                break #take only the first synset (Most frequent sense)\n",
    "        fet_vec_all.append([float(pos_score),float(neg_score)])\n",
    "    return fet_vec_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Merge two scores\n",
    "def merge_features(featureList1,featureList2):\n",
    "    # For merging two features\n",
    "    if featureList1==[]:\n",
    "        return featureList2\n",
    "    merged = []\n",
    "    for i in range(len(featureList1)):\n",
    "        m = featureList1[i]+featureList2[i]\n",
    "        merged.append(m)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#extract the sentiment labels by making positive reviews as class 1 and negative reviews as class 2\n",
    "def get_lables(data):\n",
    "    labels = []\n",
    "    for tup in data:\n",
    "        if tup[1].lower()==\"neg\":\n",
    "            labels.append(-1)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calculate_precision(prediction, actual):\n",
    "    prediction = list(prediction)\n",
    "    correct_labels = [predictions[i]  for i in range(len(predictions)) if actual[i] == predictions[i]]\n",
    "    precision = float(len(correct_labels))/float(len(prediction))\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training_unigram_features = get_unigram_features(train,vocabulary) # vocabulary extracted in the beginning\n",
    "training_swn_features = get_senti_wordnet_features(train)\n",
    "\n",
    "training_features = merge_features(training_unigram_features,training_swn_features)\n",
    "\n",
    "training_labels = get_lables(train)\n",
    "\n",
    "test_unigram_features = get_unigram_features(test,vocabulary)\n",
    "test_swn_features=get_senti_wordnet_features(test)\n",
    "test_features= merge_features(test_unigram_features,test_swn_features)\n",
    "\n",
    "test_gold_labels = get_lables(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of linear SVM classifier is:\n",
      "Training data\t0.6993333333333334\n",
      "Test data\t0.528\n"
     ]
    }
   ],
   "source": [
    "# SVM Classifier\n",
    "#Refer to : http://scikit-learn.org/stable/modules/svm.html\n",
    "from sklearn.svm import LinearSVC\n",
    "svm_classifier = LinearSVC(penalty='l2', C=0.01).fit(training_features,training_labels)\n",
    "predictions = svm_classifier.predict(training_features)\n",
    "\n",
    "print(\"Precision of linear SVM classifier is:\")\n",
    "precision = calculate_precision(predictions,training_labels)\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "predictions = svm_classifier.predict(test_features)\n",
    "precision = calculate_precision(predictions,test_gold_labels)\n",
    "print(\"Test data\\t\" + str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of linear Logistic Regression is:\n",
      "Training data\t0.8386666666666667\n",
      "Test data\t0.54\n"
     ]
    }
   ],
   "source": [
    "#logistic Regression\n",
    "from sklearn import linear_model\n",
    "Logreg=linear_model.LogisticRegression(C=1e5)\n",
    "LogClassifier=Logreg.fit(training_features,training_labels)\n",
    "predictions = LogClassifier.predict(training_features)\n",
    "\n",
    "print(\"Precision of linear Logistic Regression is:\")\n",
    "precision = calculate_precision(predictions,training_labels)\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "predictions = LogClassifier.predict(test_features)\n",
    "precision = calculate_precision(predictions,test_gold_labels)\n",
    "print(\"Test data\\t\" + str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of linear Naive Bayes Regression is:\n",
      "Training data\t0.78\n",
      "Test data\t0.522\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf=GaussianNB()\n",
    "NBClassfier=clf.fit(training_features,training_labels)\n",
    "predictions = NBClassfier.predict(training_features)\n",
    "\n",
    "print(\"Precision of linear Naive Bayes Regression is:\")\n",
    "precision = calculate_precision(predictions,training_labels)\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "predictions = NBClassfier.predict(test_features)\n",
    "precision = calculate_precision(predictions,test_gold_labels)\n",
    "print(\"Test data\\t\" + str(precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of  Decisoin Tree Classifier is:\n",
      "Training data\t0.8386666666666667\n",
      "Test data\t0.562\n"
     ]
    }
   ],
   "source": [
    "#Deicision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf=DecisionTreeClassifier()\n",
    "DTClassfier=clf.fit(training_features,training_labels)\n",
    "predictions = DTClassfier.predict(training_features)\n",
    "\n",
    "print(\"Precision of  Decisoin Tree Classifier is:\")\n",
    "precision = calculate_precision(predictions,training_labels)\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "predictions = DTClassfier.predict(test_features)\n",
    "precision = calculate_precision(predictions,test_gold_labels)\n",
    "print(\"Test data\\t\" + str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\BI\\DATASCIENCE\\Python\\anaconda\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "#Vader Analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#list of movie reviews\n",
    "sents=[]\n",
    "for i in range(0,len(documents)):\n",
    "    sents.append(' '.join(documents[i][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Vader on Training Data\n",
    "pol=[]\n",
    "for i in range(0,len(train)):\n",
    "    pol.append(sid.polarity_scores(' '.join(train[i][0])))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels_train=[]\n",
    "for i in range(0,len(pol)):\n",
    "    if pol[i]['pos'] >= 0.5:\n",
    "        lables_train.append(1)\n",
    "    else:\n",
    "        labels_train.append(-1)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Vader on Test data\n",
    "polt=[]\n",
    "for i in range(0,len(test)):\n",
    "    polt.append(sid.polarity_scores(' '.join(test[i][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for i in range(0,len(polt)):\n",
    "    if polt[i]['pos'] >= 0.5:\n",
    "        lables.append(1)\n",
    "    else:\n",
    "        labels.append(-1)\n",
    "               \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "acc_test=calculate_precision(labels,test_gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "acc_train=calculate_precision(labels_train,training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accracy of vader analysis on training data : 0.168\n",
      "Accracy of vader analysis on test data : 0.562\n"
     ]
    }
   ],
   "source": [
    "print('Accracy of vader analysis on training data',':',acc_train)\n",
    "print('Accracy of vader analysis on test data',':',acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Twitter samples example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#http://www.nltk.org/_modules/nltk/sentiment/util.html\n",
    "#http://zablo.net/blog/post/twitter-sentiment-analysis-python-scikit-word2vec-nltk-xgboost\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop=list(set(stopwords.words('English')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "categorized_tweets = ([(t, \"pos\") for t in twitter_samples.strings(\"positive_tweets.json\")] +\n",
    "                            [(t, \"neg\") for t in twitter_samples.strings(\"negative_tweets.json\")])\n",
    "\n",
    "\n",
    "smilies = [':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3', ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';(', '(', ')', 'via']\n",
    "\n",
    "\n",
    "'''categorized_tweets_tokens = []\n",
    "for tweet in categorized_tweets:\n",
    "    text = tweet[0]\n",
    "    for smiley in smilies:\n",
    "        text = re.sub(re.escape(smiley), '', text)\n",
    "    categorized_tweets_tokens.append((word_tokenize(text), tweet[1]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def processTweet(tweet):\n",
    "    # process the tweets\n",
    "\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    #remove numbers\n",
    "    tweet=re.sub('[0-9]','',tweet)\n",
    "    return tweet\n",
    "\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(categorized_tweets)):\n",
    "    categorized_tweets_remove=processTweet(categorized_tweets[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "categorized_tweets = ([[t, \"pos\"] for t in twitter_samples.strings(\"positive_tweets.json\")] +\n",
    "                            [[t, \"neg\"] for t in twitter_samples.strings(\"negative_tweets.json\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clean_tweets=[]\n",
    "for i in range(0,len(categorized_tweets)):\n",
    "    clean_tweets.append([processTweet(categorized_tweets[i][0]),categorized_tweets[i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocabulary=[w.lower() for i in range(0,len(clean_tweets)) for w in word_tokenize(clean_tweets[i][0]) if w.lower() not in stop and w.lower() not in smilies]\n",
    "vocabulary=list(set(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocabulary.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train=clean_tweets[:7000]\n",
    "test=clean_tweets[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_unigram_features(data,vocab):\n",
    "    fet_vec_all = []\n",
    "    for tup in data:\n",
    "        single_feat_vec = []\n",
    "        sent = tup[0].lower() #lowercasing the dataset\n",
    "        for v in vocab:\n",
    "            if sent.__contains__(v):\n",
    "                single_feat_vec.append(1)\n",
    "            else:\n",
    "                single_feat_vec.append(0)\n",
    "        fet_vec_all.append(single_feat_vec)\n",
    "    return fet_vec_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_senti_wordnet_features(data):\n",
    "    fet_vec_all = []\n",
    "    for tup in data:\n",
    "        sent = tup[0].lower()\n",
    "        words = sent.split()\n",
    "        pos_score = 0\n",
    "        neg_score = 0\n",
    "        for w in words:\n",
    "            senti_synsets = swn.senti_synsets(w.lower())\n",
    "            for senti_synset in senti_synsets:\n",
    "                p = senti_synset.pos_score()\n",
    "                n = senti_synset.neg_score()\n",
    "                pos_score+=p\n",
    "                neg_score+=n\n",
    "                break #take only the first synset (Most frequent sense)\n",
    "        fet_vec_all.append([float(pos_score),float(neg_score)])\n",
    "    return fet_vec_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def merge_features(featureList1,featureList2):\n",
    "    # For merging two features\n",
    "    if featureList1==[]:\n",
    "        return featureList2\n",
    "    merged = []\n",
    "    for i in range(len(featureList1)):\n",
    "        m = featureList1[i]+featureList2[i]\n",
    "        merged.append(m)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_lables(data):\n",
    "    labels = []\n",
    "    for tup in data:\n",
    "        if tup[1].lower()==\"neg\":\n",
    "            labels.append(-1)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calculate_precision(prediction, actual):\n",
    "    prediction = list(prediction)\n",
    "    correct_labels = [predictions[i]  for i in range(len(predictions)) if actual[i] == predictions[i]]\n",
    "    precision = float(len(correct_labels))/float(len(prediction))\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training_unigram_features = get_unigram_features(train,vocabulary) # vocabulary extracted in the beginning\n",
    "training_swn_features = get_senti_wordnet_features(train)\n",
    "\n",
    "training_features = merge_features(training_unigram_features,training_swn_features)\n",
    "\n",
    "training_labels = get_lables(train)\n",
    "\n",
    "test_unigram_features = get_unigram_features(test,vocabulary)\n",
    "test_swn_features=get_senti_wordnet_features(test)\n",
    "test_features= merge_features(test_unigram_features,test_swn_features)\n",
    "\n",
    "test_gold_labels = get_lables(test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#Testing of test dataset on various classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of linear SVM classifier is:\n",
      "Training data\t0.8275714285714286\n",
      "Test data\t0.7636666666666667\n"
     ]
    }
   ],
   "source": [
    "# SVM Classifier\n",
    "#Refer to : http://scikit-learn.org/stable/modules/svm.html\n",
    "from sklearn.svm import LinearSVC\n",
    "svm_classifier = LinearSVC(penalty='l2', C=0.01).fit(training_features,training_labels)\n",
    "predictions = svm_classifier.predict(training_features)\n",
    "\n",
    "print(\"Precision of linear SVM classifier is:\")\n",
    "precision = calculate_precision(predictions,training_labels)\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "predictions = svm_classifier.predict(test_features)\n",
    "precision = calculate_precision(predictions,test_gold_labels)\n",
    "print(\"Test data\\t\" + str(precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of linear Logistic Regression is:\n",
      "Training data\t0.993\n",
      "Test data\t0.6633333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#logistic Regression\n",
    "from sklearn import linear_model\n",
    "Logreg=linear_model.LogisticRegression(C=1e5)\n",
    "LogClassifier=Logreg.fit(training_features,training_labels)\n",
    "predictions = LogClassifier.predict(training_features)\n",
    "\n",
    "print(\"Precision of  Logistic Regression is:\")\n",
    "precision = calculate_precision(predictions,training_labels)\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "predictions = LogClassifier.predict(test_features)\n",
    "precision = calculate_precision(predictions,test_gold_labels)\n",
    "print(\"Test data\\t\" + str(precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of  Naive Bayes Regression is:\n",
      "Training data\t0.8124285714285714\n",
      "Test data\t0.5936666666666667\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf=GaussianNB()\n",
    "NBClassfier=clf.fit(training_features,training_labels)\n",
    "predictions = NBClassfier.predict(training_features)\n",
    "print(\"Precision of  Naive Bayes Regression is:\")\n",
    "precision = calculate_precision(predictions,training_labels)\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "predictions = NBClassfier.predict(test_features)\n",
    "precision = calculate_precision(predictions,test_gold_labels)\n",
    "print(\"Test data\\t\" + str(precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of  Decision Tree Classifier is:\n",
      "Training data\t0.9977142857142857\n",
      "Test data\t0.6776666666666666\n"
     ]
    }
   ],
   "source": [
    "#Deicision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf=DecisionTreeClassifier()\n",
    "DTClassfier=clf.fit(training_features,training_labels)\n",
    "predictions = DTClassfier.predict(training_features)\n",
    "\n",
    "print(\"Precision of  Decision Tree Classifier is:\")\n",
    "precision = calculate_precision(predictions,training_labels)\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "predictions = DTClassfier.predict(test_features)\n",
    "precision = calculate_precision(predictions,test_gold_labels)\n",
    "print(\"Test data\\t\" + str(precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\BI\\DATASCIENCE\\Python\\anaconda\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "#Vader Analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accracy of vader analysis on training data : 0.21842857142857142\n",
      "Accracy of vader analysis on test data : 0.6776666666666666\n"
     ]
    }
   ],
   "source": [
    "#list of tweets\n",
    "sents=[]\n",
    "for i in range(0,len(clean_tweets)):\n",
    "    sents.append(' '.join(clean_tweets[i][0]))\n",
    "\n",
    "#Vader on Training Data\n",
    "pol=[]\n",
    "for i in range(0,len(train)):\n",
    "    pol.append(sid.polarity_scores(' '.join(train[i][0])))\n",
    "\n",
    "\n",
    "labels_train=[]\n",
    "for i in range(0,len(pol)):\n",
    "    if pol[i]['pos'] >= 0.5:\n",
    "        lables_train.append(1)\n",
    "    else:\n",
    "        labels_train.append(-1)\n",
    "\n",
    "#Vader on Test data\n",
    "polt=[]\n",
    "for i in range(0,len(test)):\n",
    "    polt.append(sid.polarity_scores(' '.join(test[i][0])))\n",
    "\n",
    "labels=[]\n",
    "for i in range(0,len(polt)):\n",
    "    if polt[i]['pos'] >= 0.5:\n",
    "        lables.append(1)\n",
    "    else:\n",
    "        labels.append(-1)\n",
    "               \n",
    "acc_test=calculate_precision(labels,test_gold_labels)\n",
    "acc_train=calculate_precision(labels_train,training_labels)\n",
    "print('Accracy of vader analysis on training data',':',acc_train)\n",
    "print('Accracy of vader analysis on test data',':',acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# OUTPUT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Datasets        Na√Øve Bayes    SVM    Decision Tree   Logistic Regression     Vader Sentiment Analysis\n",
    "Movie_Reviews      52.2         52.8       56.2                 54                    56.2\n",
    "Twitter_Samples    59.37       76.37        67.77              66.33                   67.77\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
